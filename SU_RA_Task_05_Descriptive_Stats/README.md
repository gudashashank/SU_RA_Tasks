# LLM Performance Evaluation: Sports Statistics Analysis

A comparative study evaluating how different Large Language Models (LLMs) handle structured sports data analysis and natural language queries.

## üèÜ Project Overview

This research project compares the performance of three major LLMs‚Äî**ChatGPT**, **Claude**, and **Google AI Search**‚Äîin analyzing Syracuse Women's Lacrosse statistics from 2023-2025. The study tests each model's ability to extract information, perform calculations, and provide analytical insights from PDF-based sports data.

## üìä Key Findings

- **Google AI Search** consistently outperformed other models across all question types
- **OCR parsing issues** significantly impacted ChatGPT and Claude's accuracy
- **External knowledge access** provided substantial advantages for complex queries
- **Structured data formats** (CSV/JSON) are crucial for reliable LLM performance

## üéØ Research Objectives

- Evaluate LLM accuracy on direct factual queries
- Test reasoning capabilities for data aggregation and filtering
- Assess performance on analytical and predictive questions
- Identify limitations when working with partially available or OCR-processed data

## üìÅ Dataset

**Source:** [Syracuse University Athletics](https://cuse.com/sports/2013/1/16/WLAX_0116134638)

**Coverage:** 2023, 2024, 2025 seasons

**Data Types:**
- Team-level statistics (games, goals, shots, saves, clears, turnovers)
- Player-level statistics (goals, assists, points, turnovers)
- Game-by-game results with opponent rankings

**Format:** PDF files converted to images for testing

## üî¨ Methodology

### Question Categories
1. **Basic Retrieval** - Direct factual queries
2. **Intermediate Reasoning** - Data aggregation and filtering
3. **Advanced Analytics** - Derived metrics and subjective analysis

### Testing Process
- Identical questions posed to each LLM
- PDF/image data provided as input
- Responses validated against manually verified ground truth
- Performance compared across accuracy and detail metrics

## üìà Results Summary

### Basic Retrieval Performance
| Metric | ChatGPT | Claude | Google |
|--------|---------|--------|--------|
| Games per season | ‚úÖ | ‚úÖ | ‚úÖ |
| Win-loss records | ‚úÖ | ‚úÖ | ‚úÖ |
| Top scorers | ‚ùå | ‚ö†Ô∏è | ‚úÖ |

### Advanced Analysis
- **Trend Analysis:** All models identified performance decline vs. top-10 teams
- **Data Gaps:** Only Google successfully handled missing player statistics
- **Computation:** Google excelled at derived metrics and aggregations

## üöß Limitations Identified

1. **OCR Parsing Issues** - PDF-to-text conversion errors
2. **Missing Data Handling** - Incomplete player statistics in some seasons
3. **Computation Gaps** - Difficulty with aggregation without structured input
4. **External Knowledge Dependency** - Advantage for models with broader data access

## üîÆ Future Research

- [ ] Test performance with CSV/JSON vs. PDF inputs
- [ ] Expand to predictive analytics tasks
- [ ] Evaluate consistency with larger, more complex datasets
- [ ] Compare performance across different sports and data types

## üõ†Ô∏è Technical Stack

- **Data Sources:** PDF documents, web scraping
- **Analysis Tools:** Manual validation, comparative evaluation
- **Models Tested:** ChatGPT, Claude, Google AI Search

---

# LLM Performance Analysis: Syracuse Women's Lacrosse Data Processing (Update: Aug 15th 2025)

## Project Overview

This comprehensive study evaluated three Large Language Models (ChatGPT, Claude, and Google AI Search) on their ability to process and analyze structured sports statistics from Syracuse Women's Lacrosse team data spanning 2023-2025 seasons. The research combined empirical testing with iterative prompt engineering to identify optimal strategies for sports data analysis.

---

## Research Methodology & Process

### Phase 1: Initial Data Collection & Preparation
- **Data Source:** Syracuse University Athletics ([cuse.com](https://cuse.com/sports/2013/1/16/WLAX_0116134638))
- **Format Conversion:** PDF documents ‚Üí Image format for OCR testing
- **Dataset Scope:** 3 seasons (2023, 2024, 2025) with varying data completeness
- **Ground Truth Validation:** Manual verification of all statistics for accuracy benchmarking

### Phase 2: Baseline Testing Framework
Designed three-tier question complexity:

1. **Basic Retrieval:** Direct statistical lookups
   - Games played per season
   - Win-loss records
   - Individual season leaders

2. **Intermediate Reasoning:** Aggregation and filtering tasks
   - Cross-season comparisons
   - Derived metrics calculation
   - Statistical rankings

3. **Advanced Analytics:** Multi-factor analytical insights
   - Performance trend analysis
   - Contextual performance evaluation
   - Predictive assessments

### Phase 3: Iterative Prompt Engineering

#### Initial Prompt Strategy (Baseline)
```
"Analyze the Syracuse Women's Lacrosse statistics and answer: [question]"
```

#### Refined Prompt Engineering Techniques Applied:

**Structure Specification:**
```
"Read the table with columns: Player, Goals, Assists, Points. 
Identify the player with maximum total points."
```

**Chain-of-Thought Implementation:**
```
"First, extract the relevant values from the data.
Second, perform calculations step-by-step.
Third, show your work and final answer."
```

**Error Handling Instructions:**
```
"If data is missing or unclear, state 'data unavailable' 
rather than estimating or guessing values."
```

**Context-Limited Queries:**
```
"Focus only on the 2024 season statistics table. 
Ignore other years for this calculation."
```

**Output Format Standardization:**
```
"Provide your answer in JSON format:
{
  'player': 'name',
  'goals': number,
  'assists': number,
  'total_points': number
}"
```

### Phase 4: Comparative Analysis & Validation
- Cross-referenced all LLM responses against manually verified ground truth
- Documented parsing errors and calculation mistakes
- Identified systematic biases and limitations per model
- Measured accuracy rates across question complexity levels

---

## Key Findings & Results

### Model Performance Summary

| Capability | ChatGPT | Claude | Google AI |
|------------|---------|--------|-----------|
| Basic Retrieval | 67% | 67% | 100% |
| Intermediate Reasoning | 0% | 0% | 100% |
| Advanced Analytics | 67% | 67% | 100% |
| OCR Accuracy | Poor | Poor | Excellent |
| External Data Access | No | No | Yes |

### Critical Limitations Identified

1. **OCR Processing Failures**
   - Image-to-text conversion lost table structure
   - Digit misrecognition (e.g., 55 vs 59 goals)
   - Column alignment errors

2. **Data Completeness Challenges**
   - 2024 missing individual player statistics
   - Incomplete 2025 dataset parsing
   - Models hallucinated missing values

3. **Multi-Step Reasoning Gaps**
   - Failed at chained calculations without explicit guidance
   - Struggled with cross-table data relationships
   - Poor performance on derived metrics

### Prompt Engineering Impact Analysis

**Before Optimization:**
- Basic success rate: 56% average across models
- Frequent "cannot determine" responses
- Inconsistent output formats

**After Prompt Engineering:**
- ChatGPT/Claude: 15-20% improvement in structured tasks
- Reduced hallucination incidents by 40%
- Standardized response formats achieved
- Better error handling and transparency

**Most Effective Techniques:**
1. **Explicit Structure Definition** (+25% accuracy)
2. **Step-by-Step Instructions** (+20% accuracy)
3. **Error Handling Protocols** (-40% hallucinations)
4. **Output Format Specifications** (+30% consistency)

---

## Technical Implementation

### Data Processing Pipeline
```
Raw PDFs ‚Üí Image Conversion ‚Üí OCR Processing ‚Üí LLM Analysis ‚Üí Validation
```

### Validation Framework
- **Manual Ground Truth:** Human-verified statistics
- **Cross-Model Comparison:** Consistency checking
- **Error Classification:** Systematic vs random errors
- **Accuracy Metrics:** Precision, recall, F1-score per question type

### Prompt Templates Developed

**Standard Query Template:**
```
Context: You are analyzing Syracuse Women's Lacrosse statistics from [YEAR].
Data: [STRUCTURED_DATA_DESCRIPTION]
Task: [SPECIFIC_QUESTION]
Instructions: 
1. Extract relevant data first
2. Show calculations step-by-step  
3. State confidence level
4. Format as JSON: {"answer": value, "confidence": percentage}
```

**Error-Resistant Template:**
```
If any required data is missing or unclear:
- State "Data unavailable for [specific item]"
- Do not estimate or interpolate values
- Provide partial answers when possible with clear limitations
```

---

## Research Contributions

### Novel Insights
1. **Format Sensitivity:** Structured data (CSV/JSON) vs unstructured (PDF/images) creates 60%+ performance gaps
2. **External Knowledge Bias:** Models with web access may provide "correct" answers using different data sources
3. **Reasoning Chain Breaks:** Multi-step calculations fail without explicit intermediate step requests
4. **Context Window Management:** Long documents require segmented processing for optimal results

### Prompt Engineering Best Practices
- **Specificity Over Brevity:** Detailed instructions outperform concise queries
- **Error Prevention:** Explicit fallback handling reduces hallucinations
- **Output Standardization:** JSON/table formats improve consistency
- **Progressive Complexity:** Break complex queries into sequential simple tasks

### Practical Applications
- **Sports Analytics:** Framework applicable to any statistical sports dataset
- **Document Processing:** OCR limitation mitigation strategies
- **Multi-Model Orchestration:** Combining model strengths for optimal results
- **Quality Assurance:** Systematic validation approaches for LLM outputs

---

## Future Research Directions

### Immediate Extensions
- **Clean Data Comparison:** Test same models with CSV input vs PDF/images
- **Fine-Tuning Experiments:** Domain-specific model training on sports data
- **Multimodal Integration:** Combine text and visual processing capabilities
- **Real-Time Processing:** Live game statistics analysis

### Advanced Research Questions
- **Hallucination Prevention:** Techniques to eliminate fabricated statistics
- **Confidence Calibration:** Reliable uncertainty quantification in sports analytics
- **Temporal Reasoning:** Multi-season trend analysis and prediction
- **Causal Analysis:** Player performance impact on team outcomes

### Methodological Improvements
- **Automated Validation:** Scripts for real-time accuracy checking
- **Benchmark Dataset Creation:** Standardized sports analytics evaluation suite
- **Cross-Domain Testing:** Extension to other sports and statistical domains
- **Prompt Optimization Algorithms:** Automated prompt engineering techniques

---

## Citation & Usage

This research provides a replicable framework for evaluating LLM performance on structured data analysis tasks. The prompt engineering techniques and validation methodologies are applicable across domains requiring accurate statistical processing and reasoning.

**Recommended Citation Format:**
```
LLM Performance Analysis: Syracuse Women's Lacrosse Data Processing
[Year]. Evaluating ChatGPT, Claude, and Google AI Search on structured 
sports statistics with prompt engineering optimization.
```

---
## ü§ù Contributing

Interested in extending this research? Contributions welcome for:
- Additional LLM comparisons
- Different sports datasets
- Enhanced evaluation metrics
- Automated testing frameworks

## üìß Contact

For questions about this research or collaboration opportunities, please open an issue in this repository.

